
# ğŸ§  Dynasore â€“ Multi-Agent GenAI RAG System

A production-ready **Multi-Agent Retrieval-Augmented Generation (RAG)** system built using **LangGraph, ChromaDB, and FastAPI**, designed to answer domain-specific queries through specialized agents while sharing a common LLM backbone.

---

## âœ¨ Key Features

- ğŸ§  Multi-agent reasoning (Dino, Marine, Earth)
- ğŸ“š Domain-specific RAG using ChromaDB
- ğŸ”— LangGraph-based agent orchestration
- âš¡ FastAPI backend
- ğŸ“„ PDF / Excel / TXT ingestion
- ğŸ³ Docker & Docker Compose support
- ğŸ§© Modular, extensible architecture

---

## ğŸ—ï¸ Architecture Design (HLD & LLD)

This project follows a **layered, modular, and scalable architecture** designed for real-world GenAI applications using **Multi-Agent RAG** principles.
![HLD & LLD Architecture Diagram](sysdesign/hldlld.png)

---

## âš™ï¸ Configuration

```python
LLM_MODEL = "phi3:mini"
VECTOR_DB_DIR = "chroma_db"
CHUNK_SIZE = 500
CHUNK_OVERLAP = 50
```

---

## ğŸ§ª Local Setup

### 1ï¸âƒ£ Create Virtual Environment

```bash
python -m venv venv
```

Activate it:

```bash
# Linux / macOS
source venv/bin/activate

# Windows
venv\Scripts\activate
```

---

### 2ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

---

### 3ï¸âƒ£ Run the Application

```bash
python -m src.api.app
```

Server will start at:

```
http://localhost:5000
```

---

## ğŸ”Œ API Endpoints

### ğŸ§  Agent Query

**POST** `/api/agents/query`

```json
{
  "question": "Explain coral reefs",
  "agent": "marine"
}
```

**Available Agents**

- dino
- marine
- earth
- all â€“ executes all agents sequentially

---

### ğŸ“š RAG-Only Query

**POST** `/api/rag/query`

```json
{
  "question": "Why did dinosaurs go extinct?"
}
```

---

## ğŸ§¬ Multi-Agent Execution Flow

```
User Query
    â†“
Dino Agent
    â†“
Marine Agent
    â†“
Earth Agent
    â†“
Unified Response
```

### Architecture Notes

- Each agent maintains its own ChromaDB collection
- All agents share a single LLM
- Agents execute independently and sequentially
- Responses are aggregated into a final answer

---

## ğŸ“¥ Data Ingestion Pipeline

### Supported Formats

- PDF
- XLSX
- TXT

### Example Structure

```
data/
â”œâ”€â”€ marine_life.pdf
â”œâ”€â”€ ocean_species.xlsx
```

### Ingestion Workflow

- Files loaded at application startup
- Text chunking with overlap
- Embeddings generated
- Stored in agent-specific ChromaDB collections

---

## ğŸ³ Docker Support

### Dockerfile

```dockerfile
FROM python:3.11-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

EXPOSE 5000

CMD ["python", "-m", "src.api.app"]
```

---

### Docker Compose

```yaml
version: "3.9"

services:
  genai:
    build: .
    ports:
      - "5000:5000"
    volumes:
      - ./chroma_db:/app/chroma_db
    restart: always
```

---

## ğŸ§¯ Common Issues & Fixes

### âŒ Ollama Memory Error

Model requires more system memory

**Fix**
- Use phi3:mini
- Close heavy applications
- Reduce chunk size

---

### âŒ Slow First Response

**Reason**
- Initial embedding generation

**Fix**
- Wait for first execution
- Subsequent requests are fast

---

### âŒ Module Import Errors

Always start the app using:

```bash
python -m src.api.app
```

---

## ğŸš€ Roadmap

- JWT-based authentication
- Streaming responses
- User-level memory
- Cloud deployment (AWS / Azure)
- React frontend

---

## ğŸ‘¨â€ğŸ’» Author

**Nishant Mane**  
GenAI Engineer | RAG | LangGraph | Full-Stack Developer

---

## â­ Support

If you find this project useful:

- Star the repository
- Report issues
- Suggest enhancements
